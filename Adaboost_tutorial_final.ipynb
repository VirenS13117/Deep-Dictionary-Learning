{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaboost_tutorial_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirenS13117/Deep-Dictionary-Learning/blob/master/Adaboost_tutorial_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFNjhuptOh9Q"
      },
      "source": [
        "<h1 align=\"center\"><font color='red'> AdaBoost Guide for Beginners</font></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA3dZrsLWw9T"
      },
      "source": [
        "<h2><font color='blue'> Introduction</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bZfehBOWSci"
      },
      "source": [
        "Machine learning is all around us. Over the years, machine learning has penerated in our daily lives, whether it is weather prediction or recommending new movies/videos on Netflix/Youtube, Machine Learning is omnipresent. We are using it without even realising it. And as a data scientist, we would not want these machine learning models to be weak learners. Instead, we would want them to be strong and make accurate predictions. And, sometimes, inorder to achieve this, we end up making the Machine Learning model very complex. \\\\\n",
        "Can we make strong models without making the model too complex? Yes, we can. In this tutorial we will talk about one of the famous techniques to do this - **Adaptive Boosting** or what it is popularly called as **AdaBoost**.  Adaboost is a powerful tool that can improve the performance of any exisiting machine learning algorithm. And more importantly, it is can do this without increasing the complexity of the model significantly. AdaBoost falls under the cateogary of ensembling techniques and can boost the performance of our machine learning models. Therefore, it is a must have skill for a data scientist!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtKiLOibB2eY"
      },
      "source": [
        "First things first. Let's start from what is ensembling? Well,  in this tutorial, we will discuss high level idea of ensembling and then dicuss in depth about AdaBoost. We will learn the algorithm for AdaBoost, the mathematics behind it and also implement it using numpy. Towards the end, we will also implement AdaBoost classifier using sklearn library. The tutorial is divided into the following subsections: \\\\\n",
        "\n",
        "\n",
        "1.   Ensembling\n",
        "2.   AdaBoost\n",
        "3.   Decision Stumps\n",
        "4.   Algorithm\n",
        "5.   Mathematics behind Adaboost\n",
        "6.   Implementing AdaBoost from scratch using numpy\n",
        "7.   Implementing AdaBoost using sklearn\n",
        "8.   Additional Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cVgiR0IGER9"
      },
      "source": [
        "So, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFtIYPLW2u_"
      },
      "source": [
        "<h2><font color='blue'> Ensembling</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDALlMbvXJO8"
      },
      "source": [
        " In ensembling, the key idea is \"diversity\". Ensembling is a technique that uses a set of weak classifiers and combines them to make a strong classifier. Can we learn weak models on different parts of the feature space and combine them to form a classifier which has better overall performance on the entire feature space? Ensembling does exactly this. It combines the decisions from multiple models to improve the overall performance. A weak model  can be as simple as a classifier which is just better than random guessing. There are 3 types of ensembling techniques: \\\\\n",
        "1) Bagging \\\\\n",
        "2) Boosting \\\\\n",
        "3) Stacking \\\\\n",
        "In this tutorial, we will focus on AdaBoost which comes under Boosting. Let's first understand the Boosting and then we can dive deeper into the AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67-B6ioAT83"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1IVI-k7qd1G-Rcdfhy1D2CxfAJaMx8j2R' />\n",
        "</center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Dbi_MTW5Aj"
      },
      "source": [
        "<h2><font color='blue'> Boosting</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FupaxETW80r"
      },
      "source": [
        "We all make mistakes and we learn from them. We avoid making the same mistakes again, similarly boosting algorithm tries to build a strong machine learning model by sequentially learning from the mistakes of previous weak learners. In boosting, we add a model at each step where the newly added model corrects the mistakes of the previous models. The newly added model makes correct predictions for the points which were misclassified by previous classifiers. We keep adding classifiers until the entire training data is correctly classified.\n",
        "\n",
        "Types of Boosting Algorithms: \\\\\n",
        "1) AdaBoost (Adaptive Boosting) \\\\\n",
        "2) Gradient Tree Boosting \\\\\n",
        "3) XGBoost \\\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjLnydve0YC2"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1_lyd2lajLiRN5pp5fD7cUacvwUWInoC6' />\n",
        "</center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvC5qLVSY4SC"
      },
      "source": [
        "<h2><font color='blue'> AdaBoost</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4gGW639Y6nG"
      },
      "source": [
        "AdaBoost (Adaptive Boosting) is one of the most famous boosting technique therefore the basic principle is same i.e. combining multiple weak classifiers to build one strong classifier. AdaBoost was first introduced in the paper titled 'A Short Introduction to Boosting' by Yoav Freund and Robert Schapire.\n",
        "\n",
        "An individual classifier may not be good at accurately predicting the class label for an input. But when we progressively combine multiple weak classifiers where each new weak learner learns progressively from the mistakes of the previous models, we end up building a strong model. The classifier mentioned here could be any of your basic classifiers like Decision Trees, SVM or Logistic Regression etc.\n",
        "\n",
        "Now, what do we mean by a \"weak\" classifier? Any classifier better than random can be considered as a weak classifier. Even though it better than random but its performance is poor. For example, a weak classifier has an accuracy of 51% which is better than random and it will misclasssify points 49% of time. A weak classifier is simple and less prone to overfitting. \n",
        "\n",
        "AdaBoost is not a model in itself. Rather it can be applied on top of any classifier to learn from its shortcomings and propose a more accurate model.\n",
        "\n",
        "How does it work? \\\\\n",
        "On high level, AdaBoost does the following: \\\\\n",
        "- AdaBoost assigns weights to all the data points in the training dataset. Initially, all the data points have the same weight \\\\\n",
        "w = 1/length(training_data).\n",
        "- We create a weak classifier like a decision stump. All the data points, which are correctly classified by this stump, AdaBost decreases their weights. On the other hand, the data points which were misclassified, their weight is increased so that the next weak classifier focuses more on the misclassified points. \\\\\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bIM3Ls1l7oC"
      },
      "source": [
        "We discuss this algorithm in more details in the following sections. Before, we proceed, let's look at decision stumps as we will be using them for creating an AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyrzubXRYpN6"
      },
      "source": [
        "<h2><font color='blue'> Decision Stumps</font></h2>\n",
        "\n",
        "In this tuorial, we will apply AdaBoost on Decision Stumps. Decision Stumps are trees with only one level, they are not \"fully grown\" trees. They have 1 node and 2 leaves. In the image below, we show a decision stump based on Temperature T, if T is less than 30, it is classified as cold else hot. We will use AdaBoost technique to combine multiple stumps(weak classifiers) to create a strong classifier.\n",
        "\n",
        "A full grown tree is prone to overfitting. Stumps are simple and have high bias. A single stump is generally not a good classifier. A stump uses one variable to make a decision. The image shown below is an example of a decision stump-they can be as simple as vertical or horizontal lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO7ZOlXRMIb8"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1r2ku47o8WvrlsQXSn0ERVizIdxgcjRAB' />\n",
        "<figcaption>Decision Stump</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzn4RLTvgki"
      },
      "source": [
        "So, decision stumps can be considered as horizontal or vertical lines separating the data as shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d9Pil8inGK"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1XQob2BDRxzeiXI9eJf4_oVfiYsqoiRqj' />\n",
        "<figcaption>Vertical Decision Stump</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XDDBiK6ivKJ"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1jAYJvz7fCvtoQoxFEr7i_pjXnjwgyi-P' />\n",
        "<figcaption>Horizontal Decision Stump</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W_An0RhZHqI"
      },
      "source": [
        "Now, we are ready to go through the actual AdaBoost algorithm. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOcDsX3xZPO_"
      },
      "source": [
        "<h2><font color='blue'> Algorithm</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx4iD5JG0z-"
      },
      "source": [
        "AdaBoost assigns weight to each data point in our training data. Initially, all the data points have the same weight i.e. weights have uniform distribution. \n",
        "- In the first step, AdaBoost creates a weak classifier (it can be SVM, decision tree, logistic regression or any classifier).The error $\\epsilon$ of that classifier is given by sum (weights of incorrectly classified data points). So, AdaBoost assigns weight or vote to every classifier it creates. By vote we mean, what will be the weight and contribution of this weak classifier in the over all classifier. \\\\\n",
        "The vote value : $\\beta$ = (1/2) log((1-$\\epsilon$)/$\\epsilon$).\n",
        "- AdaBoost stores this weak classifier and it's vote.\n",
        "- In the next step, AdaBoost updates the weights of each data point in the training data. So, all the data points which were correctly classified by the previous weak classifier --> their weights are decreased. On the other hand, the data points which were incorrectly classified, their weight is increased. This encourages the next weak classifier to favour correct classifiaction of these missclassified points. This in turn encourages diversity.\n",
        "- The weights are normalized to ensure that weight of every data point is between 0 to 1.\n",
        "- Algorithm goes back to step 1\n",
        "\n",
        "After collecting enough classifiers and their corresponding vote, our AdaBoost classifier is ready. At test time, given a data point, the AdaBoost classifies based on:\n",
        "> ypred(T) = $\\sum$($\\beta_{t}$ * $h_{t}$(x))\n",
        "\n",
        "That's it! You know how AdaBoost works. More, formally, the pseudo code of the entire algorithm is shown in the next section. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EeOPGVdjwCP"
      },
      "source": [
        "<h2><font color='blue'> Mathematics behind AdaBoost</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUtPME6joqf5"
      },
      "source": [
        "To understand the mathematics behind Adaboost, let's look at the algorithm proposed by the inventors of this algorithm, Freud and Scharpire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JDfHEL7X200"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=104zPRMespmC-0U0nK5uR6DWHgDqn6Oc-' />\n",
        "</center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz44SMqsEWqO"
      },
      "source": [
        "The daigram below also shows the pipeline of AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYgETEXuJ-K4"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1rAG7rtfjumoJZBF1lt6Y7AW6sLHiEQ5m' />\n",
        "<figcaption></figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ewk0IlufLV4"
      },
      "source": [
        "<h2><font color='blue'> Implementation </font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbIbGDNIfOhl"
      },
      "source": [
        "Let's now dive into the implementation! In this section, we will create an AdaBoost classifier on a small dataset. We will be using decision stumps as the weak classifiers. We will first load the data set. Since, we do not have any hyperparmater to tune, we do not have validation data for this problem. We will use train data to train our Adaboost classifier and then test it on the testing data set. We will observe the accuracy of the test data as training progresses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CawlWRsfSgV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from numpy import linalg as LA\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puwc5sNFguzK"
      },
      "source": [
        "Let's download the data set by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toINzpYP_2vH",
        "outputId": "39e13a6f-5894-4214-a173-182269bd684b"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1RNzPW24yNfgaBq51Zwa6tHeLGlhANY83' -O \"train_adaboost.csv\"\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=13o02PNdaHEu7pRyrP0BWxntPSt6o97Sx' -O \"test_adaboost.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 02:04:20--  https://docs.google.com/uc?export=download&id=1RNzPW24yNfgaBq51Zwa6tHeLGlhANY83\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.97.138, 108.177.97.100, 108.177.97.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.97.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bpuhubvp6df5pbu7g3su6rv6246cfk4t/1617847425000/12811370926114128046/*/1RNzPW24yNfgaBq51Zwa6tHeLGlhANY83?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-04-08 02:04:21--  https://doc-14-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bpuhubvp6df5pbu7g3su6rv6246cfk4t/1617847425000/12811370926114128046/*/1RNzPW24yNfgaBq51Zwa6tHeLGlhANY83?e=download\n",
            "Resolving doc-14-30-docs.googleusercontent.com (doc-14-30-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-14-30-docs.googleusercontent.com (doc-14-30-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16834 (16K) [text/csv]\n",
            "Saving to: ‘train_adaboost.csv’\n",
            "\n",
            "train_adaboost.csv  100%[===================>]  16.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-08 02:04:21 (78.0 MB/s) - ‘train_adaboost.csv’ saved [16834/16834]\n",
            "\n",
            "--2021-04-08 02:04:21--  https://docs.google.com/uc?export=download&id=13o02PNdaHEu7pRyrP0BWxntPSt6o97Sx\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.97.138, 108.177.97.139, 108.177.97.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.97.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7h9a3o0jn5quapffgsdh69eag707bq5j/1617847425000/12811370926114128046/*/13o02PNdaHEu7pRyrP0BWxntPSt6o97Sx?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-04-08 02:04:22--  https://doc-0g-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7h9a3o0jn5quapffgsdh69eag707bq5j/1617847425000/12811370926114128046/*/13o02PNdaHEu7pRyrP0BWxntPSt6o97Sx?e=download\n",
            "Resolving doc-0g-30-docs.googleusercontent.com (doc-0g-30-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0g-30-docs.googleusercontent.com (doc-0g-30-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4233 (4.1K) [text/csv]\n",
            "Saving to: ‘test_adaboost.csv’\n",
            "\n",
            "test_adaboost.csv   100%[===================>]   4.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-08 02:04:22 (34.7 MB/s) - ‘test_adaboost.csv’ saved [4233/4233]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCaN8zXCE2Pr"
      },
      "source": [
        "We will first load the dataset using pandas and then see the first few lines of the datatset to get an idea about the datatset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg3E4W5ugUxb"
      },
      "source": [
        "complete_train = pd.read_csv('/content/train_adaboost.csv')\n",
        "complete_test = pd.read_csv('/content/test_adaboost.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZTpKMzwqgqXs",
        "outputId": "9314aabc-77b8-4cdf-9710-673a5a53bf0c"
      },
      "source": [
        "complete_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.479882</td>\n",
              "      <td>0.153095</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.810900</td>\n",
              "      <td>0.707024</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.105759</td>\n",
              "      <td>-1.588925</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.039545</td>\n",
              "      <td>-0.289559</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.062821</td>\n",
              "      <td>0.018417</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         x1        x2  label\n",
              "0 -0.479882  0.153095     -1\n",
              "1 -1.810900  0.707024      1\n",
              "2 -1.105759 -1.588925      1\n",
              "3  0.039545 -0.289559     -1\n",
              "4  0.062821  0.018417     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biQHjOQshDsy"
      },
      "source": [
        "The dataset has feature length as 2 and the output has 2 possible values {-1, 1}. Our task is a classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMPAxANDhagY"
      },
      "source": [
        "Now, our next step will be to create multiple weak classifiers. For this problem, we will be creating multiple weak decision trees, also called decision stumps. By, decision stumps, we mean that we create multiple one level decision tree. As we have seen that our features are continuous values so we need threshold value to create a weak decision tree. So let's create a class called DecisionStump. We will need 2 methods inside this class:\n",
        "*   **myStumpClassification** - This method will create a decision stump based on a thresold. It will take the data, threshold and column number(which column to use as we have 2 columns in our dataset). This method will return the pediction array from the stump.\n",
        "*   **myStumpError** - This method will return the prediction error for the stump.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fHwP7vyiRyw"
      },
      "source": [
        "We create a Decision Stump class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRh0tWKhfKgd"
      },
      "source": [
        "class DecisionStump():\n",
        "    #class for decision stump\n",
        "    def __init__(self):        \n",
        "        pass\n",
        "    def myStumpClassification(self, data, thres, col, k):\n",
        "      if (k==0):\n",
        "        resArr = np.ones((data.shape[0], 1))\n",
        "        resArr = np.where(data.iloc[:,col]<thres, -1, 1)\n",
        "        return resArr\n",
        "      else:\n",
        "        resArr = np.ones((data.shape[0], 1))\n",
        "        resArr = np.where(data.iloc[:,col]<thres, 1, -1)\n",
        "        return resArr\n",
        "    def myStumpError(self, true_val, pred_val):\n",
        "      myError = np.zeros((true_val.shape[0],))\n",
        "      myError[true_val != pred_val] = 1\n",
        "      unique, counts = np.unique(myError, return_counts=True)\n",
        "      return myError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPqYrQaRkAfy"
      },
      "source": [
        "Now we will create our AdaBoost class. This class will contain the following methods:\n",
        "\n",
        "\n",
        "*   **read_data** - This method is called from the constructor of Adaboost class. It takes the file name (in the current directory) as the input and reads the data from the file. This method returns the dataframe for train and test data.\n",
        "*   **weak_classifier** - This method finds the best stump(one level decision tree) for the data. Initially, all data points have the same weight but as training progresses, the weights of data points change and best decision stump will change accordingly. Since, for this implementation, we are using decision tree and both features are continous, we use the feature value as the threshold to create a decision stump. By best decision stump, we mean the decision stump with the least error. We keep storing these decision stumps in self.my_best_stumps for each iteration.\n",
        "*   **update_weights** - After getting the best decision stump, we update the weights of each data point. Points which were correctly classified, their weight decreases while the weight of incorrectly classified points increases.\n",
        "*   **eval_model** - This method evaluates the performance of the Adaboost on the test data set. It iterates over all the best stumps created till now, makes prediction according to each one of them and muiltplies the prediction with their corresponding weights/vote.\n",
        "*   **adaboost_train** - As the name suggests, this method trains the Adaboost classifier. It takes number of stumps or weak classifiers as the input. It creates many decision stumps, learns their weight and creates the entire classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rJeoTltZmPU"
      },
      "source": [
        "class Adaboost():\n",
        "            \n",
        "    def __init__(self, train_file, test_file):\n",
        "        #Load the data\n",
        "        self.train_data, self.test_data = self.read_data(train_file, test_file)\n",
        "        self.train_labels, self.test_labels = self.train_data.iloc[:,2], self.test_data.iloc[:,2]\n",
        "        self.train_data, self.test_data = self.train_data.iloc[:,:2], self.test_data.iloc[:,:2]\n",
        "        self.curr_dist = None\n",
        "        self.my_best_stumps = []\n",
        "        self.num_samples, self.num_features = self.train_data.shape\n",
        "\n",
        "    \n",
        "    def read_data(self, train_file, test_file):\n",
        "        #read data form file paths and return train and test data\n",
        "        return pd.read_csv(train_file), pd.read_csv(test_file)\n",
        "\n",
        "    def weak_classifier(self, true_labels):\n",
        "        df = self.train_data\n",
        "        min_err = 1000\n",
        "        bestStump = {}\n",
        "        for i in range(2): ##number of features\n",
        "          for j in range(self.train_data.shape[0]):\n",
        "            for k in range(2):\n",
        "              thres = df.iloc[j,i]\n",
        "              dstump = DecisionStump()\n",
        "              ypred = dstump.myStumpClassification(df, thres, i, k)\n",
        "              error = dstump.myStumpError(true_labels, ypred)\n",
        "              weightedError = np.multiply(error, self.curr_dist)\n",
        "              err_norm = np.sum(weightedError, axis=0)\n",
        "              if(err_norm < min_err):\n",
        "                min_err = err_norm\n",
        "                bestStump['min_err'] = err_norm\n",
        "                bestStump['col'] = i\n",
        "                bestStump['thresh'] = thres\n",
        "                bestStump['pred'] = ypred\n",
        "                bestStump['beta'] = float(0.5 * np.log((1.0 - min_err)/max(min_err, 1e-16)))\n",
        "                bestStump['row'] = j\n",
        "                bestStump['k'] = k\n",
        "        return bestStump\n",
        "\n",
        "    def update_weights(self, d_stump, true_labels, pred_val, beta_value):\n",
        "      exp_val = -1 * beta_value * true_labels\n",
        "      exp_val = np.multiply(exp_val, pred_val)\n",
        "      v1 = np.exp(exp_val)\n",
        "      self.curr_dist = np.multiply(self.curr_dist, np.exp(exp_val)) / np.sum(self.curr_dist)\n",
        "    \n",
        "    def eval_model(self):\n",
        "        #Evaluate the model\n",
        "        total_est = np.zeros((self.test_data.shape[0],))\n",
        "        for i in range(len(self.my_best_stumps)):\n",
        "          dstump = DecisionStump()\n",
        "          ypred = dstump.myStumpClassification(self.test_data, self.my_best_stumps[i]['thresh'], self.my_best_stumps[i]['col'], self.my_best_stumps[i]['k'])\n",
        "          total_est += self.my_best_stumps[i]['beta'] * ypred ##check\n",
        "        return total_est\n",
        "    \n",
        "    def adaboost_train(self, T, mydist):\n",
        "        #Train the model\n",
        "        self.curr_dist = mydist\n",
        "        self.my_best_stumps = []\n",
        "        myAcc = []\n",
        "        for i in range(T):\n",
        "          bestStump = adaboost.weak_classifier(self.train_labels)\n",
        "          adaboost.update_weights(bestStump,self.train_labels, bestStump['pred'], bestStump['beta'])\n",
        "          self.my_best_stumps.append(bestStump)\n",
        "          out = self.eval_model()\n",
        "          res_pred = np.ones((out.shape[0],))\n",
        "          res_pred = np.where(out<0, -1, 1)\n",
        "          curr_acc = accuracy_score(res_pred, self.test_labels)\n",
        "          myAcc.append(curr_acc)\n",
        "          if i%10 == 0:\n",
        "            print(\"Iteration: \", i, \" Test Accuracy: \", curr_acc)\n",
        "        return myAcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S2dIPVWrOUP"
      },
      "source": [
        "We will run the algorithm for 400 iterations. We will create 400 weak decision stumps and use it to create a strong Adaboost based classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndoqdMABkxAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d2df2b-d142-46de-db45-40e8ab1fd01c"
      },
      "source": [
        "prev_dist = np.full((400,), 0.0025)\n",
        "adaboost = Adaboost('train_adaboost.csv', 'test_adaboost.csv')\n",
        "pred_acc = adaboost.adaboost_train(400, prev_dist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  0  Test Accuracy:  0.66\n",
            "Iteration:  10  Test Accuracy:  0.9\n",
            "Iteration:  20  Test Accuracy:  0.95\n",
            "Iteration:  30  Test Accuracy:  0.95\n",
            "Iteration:  40  Test Accuracy:  0.95\n",
            "Iteration:  50  Test Accuracy:  0.95\n",
            "Iteration:  60  Test Accuracy:  0.96\n",
            "Iteration:  70  Test Accuracy:  0.96\n",
            "Iteration:  80  Test Accuracy:  0.95\n",
            "Iteration:  90  Test Accuracy:  0.96\n",
            "Iteration:  100  Test Accuracy:  0.96\n",
            "Iteration:  110  Test Accuracy:  0.96\n",
            "Iteration:  120  Test Accuracy:  0.96\n",
            "Iteration:  130  Test Accuracy:  0.96\n",
            "Iteration:  140  Test Accuracy:  0.96\n",
            "Iteration:  150  Test Accuracy:  0.96\n",
            "Iteration:  160  Test Accuracy:  0.96\n",
            "Iteration:  170  Test Accuracy:  0.96\n",
            "Iteration:  180  Test Accuracy:  0.96\n",
            "Iteration:  190  Test Accuracy:  0.96\n",
            "Iteration:  200  Test Accuracy:  0.96\n",
            "Iteration:  210  Test Accuracy:  0.96\n",
            "Iteration:  220  Test Accuracy:  0.96\n",
            "Iteration:  230  Test Accuracy:  0.96\n",
            "Iteration:  240  Test Accuracy:  0.96\n",
            "Iteration:  250  Test Accuracy:  0.96\n",
            "Iteration:  260  Test Accuracy:  0.96\n",
            "Iteration:  270  Test Accuracy:  0.96\n",
            "Iteration:  280  Test Accuracy:  0.96\n",
            "Iteration:  290  Test Accuracy:  0.96\n",
            "Iteration:  300  Test Accuracy:  0.96\n",
            "Iteration:  310  Test Accuracy:  0.96\n",
            "Iteration:  320  Test Accuracy:  0.96\n",
            "Iteration:  330  Test Accuracy:  0.96\n",
            "Iteration:  340  Test Accuracy:  0.96\n",
            "Iteration:  350  Test Accuracy:  0.97\n",
            "Iteration:  360  Test Accuracy:  0.97\n",
            "Iteration:  370  Test Accuracy:  0.96\n",
            "Iteration:  380  Test Accuracy:  0.96\n",
            "Iteration:  390  Test Accuracy:  0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxZzmGtCQaJ"
      },
      "source": [
        "We observe the training, we see that the 1st iteration the accuracy with just one stump is 66%(better than random). But as training progresses and we add more decision stumps to our Adaboost, the accuracy of the classifier improves. With help of Adaboost, we achieve 97% accuracy on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3cyLFY1k5pp"
      },
      "source": [
        "The resulting plot in shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kAYd-XfQoz46",
        "outputId": "835e9553-f3d2-4156-ca0b-4d9c063d8162"
      },
      "source": [
        "t = np.arange(400)\n",
        "plt.plot(t, pred_acc)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Test Accuracy vs Number of iterations')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8trqy7WVq78q"
      },
      "source": [
        "<h2><font color='blue'> AdaBoost using sklearn</font></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf2Gc1ovrAVp"
      },
      "source": [
        "In this section, we will implement AdaBoost using sklearn library. We will first import AdaBoostClassifier and metrics from the sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXZB6nMrJdG"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzN-bub-sTQt"
      },
      "source": [
        "The train and test data is already loaded in the Adaboost class, so we will just copy them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES1rLdHzrq37"
      },
      "source": [
        "X_train = adaboost.train_data\n",
        "y_train = adaboost.train_labels\n",
        "X_test = adaboost.test_data\n",
        "y_test = adaboost.test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYxezcMCse2P"
      },
      "source": [
        "Now, we will create our AdaBoostClassifier and fit it to the training data. The AdaBoostClassifier in Sklearn has  a parameter \"base_estimator\", this is base estimator(weak classifier) from which the boosted ensemble is built. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1 which is equivalent to a decision stump. So, in the following code, we use the default value of base estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtXDl6SHrc3V"
      },
      "source": [
        "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
        "# Training Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predicting the response for test dataset\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiTw-UtyrhU4",
        "outputId": "d8263cb1-5a06-4946-aaf6-5c57563257d3"
      },
      "source": [
        "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weuDAF5IVjZT"
      },
      "source": [
        "We observe that we obtain 97% accuracy on test data usinf sklearn as well. Now, let's discuss a few advantages and disadvantages of using AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psR--e4_Zc3G"
      },
      "source": [
        "<h2><font color='blue'> Summary</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE6zzByKlYnB"
      },
      "source": [
        "In this tutorial, we discussed about AdaBoost. We started with where it can be useful, we then discussed about ensembles and it's various types to make sure that you understand where AdaBoost falls exactly. We then talked about the AdaBoost algorithm in detail, it's pseudo code and then implemented the AdaBoost algorithm from scratch using numpy library. We also implemented an AdaBoost classifier using Sklearn library. AdaBoost is like a boon to improve the accuracy of any classification algorithms if used accurately. I hope you find this tutorial useful. If you interested in reading more on this topic, I am sharing some interesting videos and articles in the next section. Do check them out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWzWWKlgt-ll"
      },
      "source": [
        "<h2><font color='blue'> Additional Resouces on AdaBoost</font></h2>\n",
        "\n",
        "*   Original research paper : [A Short Introduction to Boosting](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)\n",
        "*   Adaboost video: [Adaboost clearly explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n",
        "*   [Sklearn Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
        "\n",
        "\n"
      ]
    }
  ]
}